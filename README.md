# Text-generation-using-GPT-2

Developed by OpenAI, GPT-2 is a large-scale transformer-based language model. OpenAI trained it on a large corpus of text: 8 million high-quality web pages.

“GPT-2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the “zero-shot” setting. GPT-2 outperforms models trained on domain-specific data sets (e.g. Wikipedia, news, books) when evaluated on those same data sets.” - OpenAI Team

We use GPT-2 on many language modeling tasks such as machine translation, summarizing and question answering. It has shown a high level of competitive performance compared to the models trained for a specific purpose or domain.

“We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.” - OpenAI Team

Dataset download link: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

Happy Learning!
