{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0j4AEj7WJYz"
   },
   "source": [
    "# Text Generation using GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OFVW_HqhXcu"
   },
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GE243dPaG1Kp"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPz3a8khG2E0"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnwcMpKZhwB_"
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZwMfmmCG2J7"
   },
   "outputs": [],
   "source": [
    "# Download the Shakespeare's text.\n",
    "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "feeMzUOaW3XB"
   },
   "source": [
    "# Fine tuning for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSQnTv70G2WM"
   },
   "outputs": [],
   "source": [
    "!python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train \\\n",
    "    --train_data_file='input.txt' \\\n",
    "    --per_gpu_train_batch_size=1 \\\n",
    "    --save_steps=-1 \\\n",
    "    --num_train_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIhBZ7rLXFQe"
   },
   "source": [
    "# Loading Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaXpzPq-IOnN"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('/output')\n",
    "model = GPT2LMHeadModel.from_pretrained('/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJN_4O4YXamO"
   },
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVn4mv_GKWhC"
   },
   "source": [
    "## Greedy Search\n",
    "This is a very basic searching algorithm which selects the word with highest probability as its next word and doesn't use other words with lesser probability.\n",
    "The code for implementing greedy search with our model is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cz0yxhjCJEBm"
   },
   "outputs": [],
   "source": [
    "ids = tokenizer.encode('[BOS] The King must leave the throne now . [EOS]',\n",
    "                      return_tensors='pt')\n",
    "\n",
    "greedy_outputs = model.generate(ids, max_length=300)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, greedy_output in enumerate(greedy_outputs):\n",
    "  print(\"\\n\"+\"===\"*10)\n",
    "  print(\"{}: {}\".format(i+1, tokenizer.decode(greedy_output, skip_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rllW7EwPMV2O"
   },
   "source": [
    "## Beam Search\n",
    "It is a search algorithm which considers the probabilities of consequent no (num_beams) of words not like greedy search which simply selects word with highest probability. It then multiplies these probabilities with the previous ones for each case. Then, it selects the sequence of words which had higher overall probability after multiplication.\n",
    "\n",
    "The code for implementing beam search with our model is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H41cbQ2iOG65"
   },
   "source": [
    "We set num_beams > 1 and early_stopping=True so that generation is finished when all beam hypotheses reached the endprompts token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmANZqLXJGt9"
   },
   "outputs": [],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    ids, \n",
    "    max_length=300, \n",
    "    num_beams=4, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfF7Q8-6RddF"
   },
   "source": [
    "# Sampling\n",
    "Sampling means randomly picking the next word according to its conditional probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cDU-YE0RY3y"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUC74nGbOa_c"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "sample_output = model.generate(\n",
    "    ids, \n",
    "    do_sample=True, \n",
    "    max_length=300\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9P1KqkRx3L"
   },
   "source": [
    "As we can see it produce much better results than previous ones and the text is also starting to make some sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnMUEgxrS1MR"
   },
   "source": [
    "## Top-K Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1A3CC4JTwt4"
   },
   "source": [
    "In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NoAxdcZObHC"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output2 = model.generate(\n",
    "    ids, \n",
    "    do_sample=True, \n",
    "    max_length=300, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output2[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9oiwPsjl3n5"
   },
   "source": [
    "Now, after implementing top-k sampling, we should try out top-p sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-aYt_FOUTx6"
   },
   "source": [
    "## Top-p (Nucleus) sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M75hpAZeUshq"
   },
   "source": [
    "It is selecting the highest probability tokens whose cumulative probability mass\n",
    "exceeds the pre-chosen threshold p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQzKSp3fUQ1E"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output3 = model.generate(\n",
    "    ids, \n",
    "    do_sample=True, \n",
    "    max_length=300, \n",
    "    top_p=0.92,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output3[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fb8kBnOwVGfk"
   },
   "source": [
    "## Combining Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdJPuQuBUQyD"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 \n",
    "final_outputs = model.generate(\n",
    "    ids,\n",
    "    do_sample=True, \n",
    "    max_length=300, \n",
    "    top_k=40, \n",
    "    top_p=0.95, \n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, final_output in enumerate(final_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(final_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vijay - Text Generation - Final - 2 epoch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
